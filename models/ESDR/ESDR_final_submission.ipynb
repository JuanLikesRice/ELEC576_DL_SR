{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Turog\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\cuda\\memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms.functional import resize\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "torch.cuda.reset_peak_memory_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = out * 0.1\n",
    "        return out + residual\n",
    "\n",
    "class EDSR(nn.Module):\n",
    "    def __init__(self, scale_factor, num_channels=3, num_features=64, num_residuals=32):\n",
    "        super(EDSR, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.conv1 = nn.Conv2d(num_channels, num_features, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(num_features) for _ in range(num_residuals)]\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(num_features, num_features, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(num_features, num_features * (scale_factor ** 2), kernel_size=3, stride=1, padding=1),\n",
    "            nn.PixelShuffle(scale_factor),\n",
    "            nn.Conv2d(num_features, num_channels, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.residual_blocks(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.upsample(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision.transforms.functional import gaussian_blur\n",
    "\n",
    "def denormalize(tensor):\n",
    "    \"\"\"Convert [-1, 1] range back to [0, 1] for visualization.\"\"\"\n",
    "    return torch.clamp(tensor * 0.5 + 0.5, 0, 1)\n",
    "\n",
    "class SuperResolutionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, patch_size=128, scale_factor=2, max_patches=16, seed=42):\n",
    "        self.root_dir = root_dir\n",
    "        self.patch_size = patch_size\n",
    "        self.lr_patch_size = patch_size // scale_factor\n",
    "        self.image_list = os.listdir(root_dir)\n",
    "        self.scale_factor = scale_factor\n",
    "        self.max_patches = max_patches  \n",
    "        self.normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.image_list[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        width, height = image.size\n",
    "\n",
    "        hr_patches = []\n",
    "        lr_patches = []\n",
    "        for y in range(0, height, self.patch_size):\n",
    "            for x in range(0, width, self.patch_size):\n",
    "                hr_patch = image.crop((x, y, x + self.patch_size, y + self.patch_size))\n",
    "\n",
    "                if hr_patch.size[0] != self.patch_size or hr_patch.size[1] != self.patch_size:\n",
    "                    continue\n",
    "\n",
    "                lr_patch = resize(hr_patch, (self.lr_patch_size, self.lr_patch_size), interpolation=Image.BILINEAR)\n",
    "\n",
    "                hr_patch_tensor = self.normalize(transforms.ToTensor()(hr_patch))\n",
    "                lr_patch_tensor = self.normalize(transforms.ToTensor()(lr_patch))\n",
    "                hr_patches.append(hr_patch_tensor)\n",
    "                lr_patches.append(lr_patch_tensor)\n",
    "\n",
    "                blur_kernel_size = random.choice([3, 5])\n",
    "                lr_patch_blur = gaussian_blur(lr_patch_tensor, kernel_size=blur_kernel_size)\n",
    "                hr_patches.append(hr_patch_tensor) \n",
    "                lr_patches.append(lr_patch_blur)\n",
    "\n",
    "                noise = torch.randn_like(lr_patch_tensor) * 0.1 \n",
    "                lr_patch_noisy = torch.clamp(lr_patch_tensor + noise, 0, 1)\n",
    "                hr_patches.append(hr_patch_tensor) \n",
    "                lr_patches.append(lr_patch_noisy)\n",
    "\n",
    "        while len(hr_patches) < self.max_patches:\n",
    "            hr_patches.append(torch.zeros((3, self.patch_size, self.patch_size)))\n",
    "            lr_patches.append(torch.zeros((3, self.lr_patch_size, self.lr_patch_size)))\n",
    "\n",
    "        hr_patches = hr_patches[:self.max_patches]\n",
    "        lr_patches = lr_patches[:self.max_patches]\n",
    "\n",
    "        if len(hr_patches) == 0 or len(lr_patches) == 0:\n",
    "            raise ValueError(f\"No valid patches for image {img_path}\")\n",
    "\n",
    "        return torch.stack(lr_patches), torch.stack(hr_patches)\n",
    "\n",
    "dataset = SuperResolutionDataset(root_dir=r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\combined_largest_images_rd\")#largest_images\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Some Paramters ##\n",
    "\n",
    "\n",
    "scale_factor = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EDSR(scale_factor=scale_factor).to(device) \n",
    "criterion = nn.MSELoss().to(device)                \n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "from torch.optim import RMSprop\n",
    "optimizer = RMSprop(model.parameters(), lr=1e-4, alpha=0.9, weight_decay=1e-5)\n",
    "print(model.conv1.weight.device)  # Should print: cuda:0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms.functional import pad\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.makedirs(\"chkt_v6\", exist_ok=True) \n",
    "os.makedirs(\"progress_v6\", exist_ok=True)  \n",
    "\n",
    "def pad_image(image, patch_size):\n",
    "    width, height = image.size\n",
    "    pad_width = (patch_size - width % patch_size) % patch_size\n",
    "    pad_height = (patch_size - height % patch_size) % patch_size\n",
    "    padding = (0, 0, pad_width, pad_height)  # (left, top, right, bottom)\n",
    "    padded_image = pad(image, padding, fill=0)\n",
    "    return padded_image, padding\n",
    "\n",
    "num_epochs = 15\n",
    "loss_values = []  \n",
    "patch_size = 128  \n",
    "scale_factor = 2  \n",
    "lr_patch_size = patch_size // scale_factor  \n",
    "\n",
    "test_image_path = r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\combined_largest_images_test_model\\HR_output.PNG\" \n",
    "test_image = Image.open(test_image_path).convert(\"RGB\")\n",
    "test_width, test_height = test_image.size\n",
    "\n",
    "padded_test_image, padding = pad_image(test_image, patch_size)\n",
    "padded_width, padded_height = padded_test_image.size\n",
    "\n",
    "test_patches = []\n",
    "for y in range(0, padded_height, patch_size):\n",
    "    for x in range(0, padded_width, patch_size):\n",
    "        patch = padded_test_image.crop((x, y, x + patch_size, y + patch_size))\n",
    "        lr_patch = resize(patch, (lr_patch_size, lr_patch_size), interpolation=Image.BILINEAR)\n",
    "        test_patches.append(transforms.ToTensor()(lr_patch).unsqueeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 81/81 [03:46<00:00,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 5.3279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████| 81/81 [03:39<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 3.5021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████| 81/81 [03:36<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/15], Loss: 2.6821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████| 81/81 [03:37<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/15], Loss: 2.4325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████| 81/81 [03:37<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/15], Loss: 2.2653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████| 81/81 [2:55:29<00:00, 129.99s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/15], Loss: 2.1388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|██████████| 81/81 [04:04<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/15], Loss: 2.0593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|██████████| 81/81 [03:39<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/15], Loss: 1.9656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|██████████| 81/81 [04:06<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/15], Loss: 1.9501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|██████████| 81/81 [03:44<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/15], Loss: 1.8936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|██████████| 81/81 [03:43<00:00,  2.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/15], Loss: 1.8531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|██████████| 81/81 [03:36<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/15], Loss: 1.8189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|██████████| 81/81 [03:41<00:00,  2.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/15], Loss: 1.7807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|██████████| 81/81 [03:38<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/15], Loss: 1.7691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|██████████| 81/81 [03:38<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Loss: 1.7326\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms.functional import pad\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.makedirs(\"chkt_v6\", exist_ok=True) \n",
    "os.makedirs(\"progress_v6\", exist_ok=True)  \n",
    "\n",
    "def pad_image(image, patch_size):\n",
    "    width, height = image.size\n",
    "    pad_width = (patch_size - width % patch_size) % patch_size\n",
    "    pad_height = (patch_size - height % patch_size) % patch_size\n",
    "    padding = (0, 0, pad_width, pad_height)  # (left, top, right, bottom)\n",
    "    padded_image = pad(image, padding, fill=0)\n",
    "    return padded_image, padding\n",
    "\n",
    "num_epochs = 15\n",
    "loss_values = []  \n",
    "patch_size = 128  \n",
    "scale_factor = 2  \n",
    "lr_patch_size = patch_size // scale_factor  \n",
    "\n",
    "test_image_path = r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\combined_largest_images_test_model\\HR_output.PNG\" \n",
    "test_image = Image.open(test_image_path).convert(\"RGB\")\n",
    "test_width, test_height = test_image.size\n",
    "\n",
    "padded_test_image, padding = pad_image(test_image, patch_size)\n",
    "padded_width, padded_height = padded_test_image.size\n",
    "\n",
    "test_patches = []\n",
    "for y in range(0, padded_height, patch_size):\n",
    "    for x in range(0, padded_width, patch_size):\n",
    "        patch = padded_test_image.crop((x, y, x + patch_size, y + patch_size))\n",
    "        lr_patch = resize(patch, (lr_patch_size, lr_patch_size), interpolation=Image.BILINEAR)\n",
    "        test_patches.append(transforms.ToTensor()(lr_patch).unsqueeze(0))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    for lr_patches, hr_patches in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        batch_size, num_patches, channels, lr_height, lr_width = lr_patches.shape\n",
    "        _, _, _, hr_height, hr_width = hr_patches.shape\n",
    "\n",
    "        lr_patches = lr_patches.view(-1, channels, lr_height, lr_width).to(device)\n",
    "        hr_patches = hr_patches.view(-1, channels, hr_height, hr_width).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        sr_patches = model(lr_patches)\n",
    "\n",
    "        loss = criterion(sr_patches, hr_patches)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    loss_values.append(epoch_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), f\"chkt_v6/edsr_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    model.eval()\n",
    "    sr_test_patches = []\n",
    "    with torch.no_grad():\n",
    "        for patch in test_patches:\n",
    "            patch = patch.to(device)\n",
    "            sr_test_patches.append(model(patch).squeeze(0).cpu())\n",
    "\n",
    "    sr_test_image = torch.zeros((3, padded_height, padded_width))  # Use padded dimensions\n",
    "    patch_idx = 0\n",
    "    for y in range(0, padded_height, patch_size):\n",
    "        for x in range(0, padded_width, patch_size):\n",
    "            # sr_test_image[:, y:y+patch_size, x:x+patch_size] = sr_test_patches[patch_idx]\n",
    "            sr_test_image[:, y:y+patch_size, x:x+patch_size] = denormalize(sr_test_patches[patch_idx])\n",
    "\n",
    "            patch_idx += 1\n",
    "\n",
    "    sr_test_image = sr_test_image[:, :test_height, :test_width]\n",
    "\n",
    "    patch_idx = 0  # Choose a specific patch to visualize (adjust as needed)\n",
    "    low_res_patch = test_patches[patch_idx].squeeze(0).cpu()\n",
    "    super_res_patch = sr_test_patches[patch_idx].cpu()\n",
    "    high_res_patch = transforms.ToTensor()(padded_test_image.crop((0, 0, patch_size, patch_size)))\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Low-Resolution Patch\")\n",
    "    # plt.imshow(transforms.ToPILImage()(low_res_patch))\n",
    "    plt.imshow(transforms.ToPILImage()(denormalize(low_res_patch)))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(f\"Super-Resolved Patch (Epoch {epoch+1})\")\n",
    "    # plt.imshow(transforms.ToPILImage()(super_res_patch))\n",
    "    plt.imshow(transforms.ToPILImage()(denormalize(super_res_patch)))\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"High-Resolution Patch\")\n",
    "\n",
    "    plt.imshow(transforms.ToPILImage()(high_res_patch))\n",
    "    # plt.imshow(transforms.ToPILImage()(denormalize(high_res_patch)))\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"progress_v6/epoch_{epoch+1}_patch_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()  # Close the figure\n",
    "\n",
    "    plt.figure(figsize=(16, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(f\"Low-Resolution Input ({lr_patch_size}x{lr_patch_size} patches)\")\n",
    "\n",
    "    lr_test_reconstructed = torch.zeros((3, padded_height // scale_factor, padded_width // scale_factor))\n",
    "    patch_idx = 0\n",
    "    for y in range(0, padded_height, patch_size):\n",
    "        for x in range(0, padded_width, patch_size):\n",
    "            lr_patch = test_patches[patch_idx].squeeze(0).cpu()\n",
    "            lr_test_reconstructed[:, y // scale_factor:(y + patch_size) // scale_factor,\n",
    "                                  x // scale_factor:(x + patch_size) // scale_factor] = lr_patch\n",
    "            patch_idx += 1\n",
    "\n",
    "    lr_test_reconstructed = lr_test_reconstructed[:, :test_height // scale_factor, :test_width // scale_factor]\n",
    "    # plt.imshow(transforms.ToPILImage()(lr_test_reconstructed))\n",
    "    plt.imshow(transforms.ToPILImage()(denormalize(lr_test_reconstructed)))\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sr_height, sr_width = sr_test_image.shape[1:] \n",
    "    plt.title(f\"Super-Resolved Output (Epoch {epoch+1}, {sr_width}x{sr_height})\")\n",
    "    plt.imshow(transforms.ToPILImage()(denormalize(sr_test_image)))\n",
    "    # plt.imshow(transforms.ToPILImage()(sr_test_image))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(f\"Original High-Resolution ({test_width}x{test_height})\")\n",
    "    plt.imshow(test_image)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"progress_v6/epoch_{epoch+1}_reconstruction_comparison.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close() \n",
    "\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, len(loss_values) + 1), loss_values, marker='o', label=\"Training Loss\")\n",
    "    plt.title(\"Training Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"progress_v6/loss_plot.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference  Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Turog\\AppData\\Local\\Temp\\ipykernel_15116\\3759285895.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved comparison for '0745' to inference_results_v6\\0745_comparison.png\n",
      "PSNR Results: Bicubic = 24.92 dB, EDSR = 24.07 dB\n",
      "Saved comparison for '0746' to inference_results_v6\\0746_comparison.png\n",
      "PSNR Results: Bicubic = 29.62 dB, EDSR = 25.33 dB\n",
      "Saved comparison for '0747' to inference_results_v6\\0747_comparison.png\n",
      "PSNR Results: Bicubic = 23.89 dB, EDSR = 25.19 dB\n",
      "Saved comparison for '0748' to inference_results_v6\\0748_comparison.png\n",
      "PSNR Results: Bicubic = 20.57 dB, EDSR = 24.03 dB\n",
      "Saved comparison for '0749' to inference_results_v6\\0749_comparison.png\n",
      "PSNR Results: Bicubic = 30.30 dB, EDSR = 27.54 dB\n",
      "Saved comparison for 'HR_output' to inference_results_v6\\HR_output_comparison.png\n",
      "PSNR Results: Bicubic = 21.19 dB, EDSR = 24.35 dB\n",
      "Saved comparison for '1702053632133310' to inference_results_v6\\1702053632133310_comparison.png\n",
      "PSNR Results: Bicubic = 20.15 dB, EDSR = 23.26 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import math\n",
    "from torchvision.transforms.functional import pad, resize, to_pil_image, to_tensor\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define normalization and denormalization\n",
    "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "def denormalize(tensor):\n",
    "    \"\"\"Denormalize the tensor from [-1, 1] to [0, 1].\"\"\"\n",
    "    return torch.clamp(tensor * 0.5 + 0.5, 0, 1)\n",
    "\n",
    "def pad_image(image, patch_size):\n",
    "    \"\"\"Pad image to make dimensions divisible by patch size.\"\"\"\n",
    "    width, height = image.size\n",
    "    pad_width = (patch_size - width % patch_size) % patch_size\n",
    "    pad_height = (patch_size - height % patch_size) % patch_size\n",
    "    padding = (0, 0, pad_width, pad_height)\n",
    "    padded_image = pad(image, padding, fill=0)\n",
    "    return padded_image, padding\n",
    "\n",
    "def calculate_psnr(img1, img2):\n",
    "    \"\"\"Calculate PSNR between two images.\"\"\"\n",
    "    mse = torch.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')  # Infinite PSNR (perfect match)\n",
    "    return 20 * math.log10(1.0 / math.sqrt(mse))\n",
    "\n",
    "def psnr(pred, target):\n",
    "    \"\"\"Compute the PSNR (Peak Signal-to-Noise Ratio) between two tensors.\"\"\"\n",
    "    mse = F.mse_loss(pred, target)\n",
    "    if mse == 0:\n",
    "        return 5.5\n",
    "    return 20 * math.log10(1.0 / math.sqrt(mse.item()))\n",
    "\n",
    "def perform_inference_comparison_EDSR(\n",
    "    model_path, image_paths, output_dir=\"edsr_inference_results\", patch_size=128, scale_factor=2\n",
    "):\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load trained EDSR model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = EDSR(scale_factor=scale_factor).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        original_width, original_height = image.size\n",
    "\n",
    "        # Pad the image\n",
    "        padded_image, padding = pad_image(image, patch_size)\n",
    "        padded_width, padded_height = padded_image.size\n",
    "\n",
    "        # Prepare patches\n",
    "        lr_patches, hr_patches = [], []\n",
    "        for y in range(0, padded_height, patch_size):\n",
    "            for x in range(0, padded_width, patch_size):\n",
    "                patch = padded_image.crop((x, y, x + patch_size, y + patch_size))\n",
    "                lr_patch = resize(patch, (patch_size // scale_factor, patch_size // scale_factor), interpolation=Image.BICUBIC)\n",
    "                lr_patches.append(normalize(to_tensor(lr_patch)).unsqueeze(0).to(device))\n",
    "                hr_patches.append(normalize(to_tensor(patch)).unsqueeze(0).to(device))\n",
    "\n",
    "        # Perform inference\n",
    "        sr_patches, bicubic_patches = [], []\n",
    "        total_sr_psnr, total_bicubic_psnr = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for lr_patch, hr_patch in zip(lr_patches, hr_patches):\n",
    "                sr_patch = model(lr_patch).squeeze(0)\n",
    "                # bicubic_patch = torch.nn.functional.interpolate(lr_patch, scale_factor=scale_factor, mode=\"bicubic\")\n",
    "\n",
    "                bicubic_patch = F.interpolate(\n",
    "                    lr_patch, scale_factor=scale_factor, mode=\"bilinear\", align_corners=True\n",
    "                ).squeeze(0)\n",
    "\n",
    "                sr_patches.append(denormalize(sr_patch.cpu()))\n",
    "                bicubic_patches.append(denormalize(bicubic_patch.squeeze(0).cpu()))\n",
    "                total_sr_psnr += calculate_psnr(denormalize(sr_patch), denormalize(hr_patch.squeeze(0)))\n",
    "                total_bicubic_psnr += psnr(bicubic_patch.squeeze(0),hr_patch.squeeze(0))\n",
    "\n",
    "\n",
    "        avg_sr_psnr = total_sr_psnr / len(hr_patches)\n",
    "        avg_bicubic_psnr = total_bicubic_psnr / len(hr_patches)\n",
    "\n",
    "        # Reconstruct images\n",
    "        sr_image = torch.zeros((3, padded_height, padded_width))\n",
    "        bicubic_image = torch.zeros((3, padded_height, padded_width))\n",
    "        patch_idx = 0\n",
    "        for y in range(0, padded_height, patch_size):\n",
    "            for x in range(0, padded_width, patch_size):\n",
    "                sr_image[:, y:y+patch_size, x:x+patch_size] = sr_patches[patch_idx]\n",
    "                bicubic_image[:, y:y+patch_size, x:x+patch_size] = bicubic_patches[patch_idx]\n",
    "                patch_idx += 1\n",
    "        sr_image = sr_image[:, :original_height, :original_width]\n",
    "        bicubic_image = bicubic_image[:, :original_height, :original_width]\n",
    "\n",
    "\n",
    "\n",
    "        # test_image = Image.open(test_image_path).convert(\"RGB\")\n",
    "        padded_test_image, padding = pad_image(test_image, patch_size)\n",
    "        padded_width, padded_height = padded_test_image.size\n",
    "        test_patches = []\n",
    "        hr_patches = []\n",
    "        for y in range(0, padded_height, patch_size):\n",
    "            for x in range(0, padded_width, patch_size):\n",
    "                patch = padded_test_image.crop((x, y, x + patch_size, y + patch_size))\n",
    "                lr_patch = resize(patch, (patch_size // scale_factor, patch_size // scale_factor), interpolation=Image.BILINEAR)\n",
    "                test_patches.append(normalize(to_tensor(lr_patch)).unsqueeze(0))  \n",
    "                hr_patches.append(normalize(to_tensor(patch)).unsqueeze(0)) \n",
    "\n",
    "        test_patches = [patch.to(device) for patch in test_patches]\n",
    "        hr_patches = [patch.to(device) for patch in hr_patches]\n",
    "\n",
    "        bilinear_patches = []\n",
    "        total_bilinear_psnr = 0.0\n",
    "        with torch.no_grad():\n",
    "            for lr_patch, hr_patch in zip(test_patches, hr_patches):\n",
    "                bilinear_patch = F.interpolate(\n",
    "                    lr_patch, scale_factor=scale_factor, mode=\"bilinear\", align_corners=False\n",
    "                ).squeeze(0)\n",
    "                bilinear_patches.append(bilinear_patch)\n",
    "                total_bilinear_psnr += psnr(bilinear_patch, hr_patch.squeeze(0))\n",
    "        total_bicubic_psnr = total_bilinear_psnr / len(test_patches)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Save comparison\n",
    "        plt.figure(figsize=(30, 15))\n",
    "\n",
    "        # Low-Resolution Input\n",
    "        plt.subplot(1, 4, 1)\n",
    "        plt.title(\"Low-Resolution Input\")\n",
    "        lr_image = resize(image, (original_height // scale_factor, original_width // scale_factor), interpolation=Image.BICUBIC)\n",
    "        plt.imshow(lr_image)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Bicubic Upscaling\n",
    "        plt.subplot(1, 4, 2)\n",
    "        plt.title(f\"Bicubic PSNR: {avg_bicubic_psnr:.2f} dB\")\n",
    "        plt.imshow(to_pil_image(bicubic_image))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Super-Resolved Output (EDSR)\n",
    "        plt.subplot(1, 4, 3)\n",
    "        plt.title(f\"EDSR PSNR: {avg_sr_psnr:.2f} dB\")\n",
    "        plt.imshow(to_pil_image(sr_image))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Original High-Resolution\n",
    "        plt.subplot(1, 4, 4)\n",
    "        plt.title(\"Original High-Resolution\")\n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        output_path = os.path.join(output_dir, f\"{image_name}_comparison.png\")\n",
    "        plt.savefig(output_path, dpi=500, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"Saved comparison for '{image_name}' to {output_path}\")\n",
    "        print(f\"PSNR Results: Bicubic = {avg_bicubic_psnr:.2f} dB, EDSR = {avg_sr_psnr:.2f} dB\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_list = [\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0745.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0746.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0747.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0748.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0749.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\HR_output.PNG\", r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\waves\\1702053632133310.png\"]\n",
    "perform_inference_comparison_EDSR(\"chkt_v6/edsr_epoch_15.pth\", image_list, scale_factor=2,output_dir=\"inference_results_v6\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import math\n",
    "from torchvision.transforms.functional import pad, resize, to_pil_image, to_tensor\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define normalization and denormalization\n",
    "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "def denormalize(tensor):\n",
    "    \"\"\"Denormalize the tensor from [-1, 1] to [0, 1].\"\"\"\n",
    "    return torch.clamp(tensor * 0.5 + 0.5, 0, 1)\n",
    "\n",
    "def calculate_psnr(img1, img2):\n",
    "    \"\"\"Calculate PSNR between two images.\"\"\"\n",
    "    mse = torch.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')  # Infinite PSNR (perfect match)\n",
    "    return 20 * math.log10(1.0 / math.sqrt(mse))\n",
    "\n",
    "def perform_inference_single_patch_EDSR(\n",
    "    model_path, image_paths, output_dir=\"edsr_best_patch_results\", patch_size=128, scale_factor=2\n",
    "):\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load trained EDSR model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = EDSR(scale_factor=scale_factor).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        original_width, original_height = image.size\n",
    "\n",
    "        # Pad the image\n",
    "        padded_image, _ = pad_image(image, patch_size)\n",
    "        padded_width, padded_height = padded_image.size\n",
    "\n",
    "        # Prepare patches\n",
    "        lr_patches, hr_patches = [], []\n",
    "        for y in range(0, padded_height, patch_size):\n",
    "            for x in range(0, padded_width, patch_size):\n",
    "                patch = padded_image.crop((x, y, x + patch_size, y + patch_size))\n",
    "                lr_patch = resize(patch, (patch_size // scale_factor, patch_size // scale_factor), interpolation=Image.BICUBIC)\n",
    "                lr_patches.append(normalize(to_tensor(lr_patch)).unsqueeze(0).to(device))\n",
    "                hr_patches.append(normalize(to_tensor(patch)).unsqueeze(0).to(device))\n",
    "\n",
    "        # Find the patch with the highest PSNR\n",
    "        highest_psnr = 0.0\n",
    "        best_patch_index = 0\n",
    "        bilinear_psnr_best = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, (lr_patch, hr_patch) in enumerate(zip(lr_patches, hr_patches)):\n",
    "                sr_patch = model(lr_patch).squeeze(0)\n",
    "                bilinear_patch = F.interpolate(\n",
    "                    lr_patch, scale_factor=scale_factor, mode=\"bilinear\", align_corners=False\n",
    "                ).squeeze(0)\n",
    "                psnr_value = calculate_psnr(denormalize(sr_patch), denormalize(hr_patch.squeeze(0)))\n",
    "                bilinear_psnr = calculate_psnr(bilinear_patch, hr_patch.squeeze(0))\n",
    "\n",
    "                if psnr_value > highest_psnr:\n",
    "                    highest_psnr = psnr_value\n",
    "                    bilinear_psnr_best = bilinear_psnr\n",
    "                    best_patch_index = i\n",
    "\n",
    "        # Extract the best patch\n",
    "        lr_patch_best = denormalize(lr_patches[best_patch_index].squeeze(0).cpu())\n",
    "        sr_patch_best = denormalize(model(lr_patches[best_patch_index]).squeeze(0).cpu())\n",
    "        hr_patch_best = denormalize(hr_patches[best_patch_index].squeeze(0).cpu())\n",
    "\n",
    "        # Bilinear Upscale\n",
    "        with torch.no_grad():\n",
    "            bilinear_patch = F.interpolate(\n",
    "                lr_patches[best_patch_index], scale_factor=scale_factor, mode=\"bilinear\", align_corners=True\n",
    "            ).squeeze(0)\n",
    "            bilinear_patch_best = denormalize(bilinear_patch.cpu())\n",
    "\n",
    "        # Save comparison\n",
    "        plt.figure(figsize=(20, 5))\n",
    "\n",
    "        # Low-Resolution Patch\n",
    "        plt.subplot(1, 4, 1)\n",
    "        plt.title(\"Low-Resolution Patch\")\n",
    "        plt.imshow(to_pil_image(lr_patch_best))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Bilinear Upscale\n",
    "        if bilinear_psnr_best < highest_psnr:\n",
    "            plt.subplot(1, 4, 2)\n",
    "            plt.title(f\"Bilinear PSNR: {bilinear_psnr_best:.2f} dB\")\n",
    "            plt.imshow(to_pil_image(bilinear_patch_best))\n",
    "            plt.axis(\"off\")\n",
    "        else:\n",
    "            plt.subplot(1, 4, 2)\n",
    "            plt.title(\"Bilinear Upscale\")\n",
    "            plt.imshow(to_pil_image(bilinear_patch_best))\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "        # Super-Resolved Output (EDSR)\n",
    "        plt.subplot(1, 4, 3)\n",
    "        plt.title(f\"EDSR PSNR: {highest_psnr:.2f} dB\")\n",
    "        plt.imshow(to_pil_image(sr_patch_best))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Original High-Resolution Patch\n",
    "        plt.subplot(1, 4, 4)\n",
    "        plt.title(\"Original High-Resolution Patch\")\n",
    "        plt.imshow(to_pil_image(hr_patch_best))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        output_path = os.path.join(output_dir, f\"{image_name}_best_patch.png\")\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"Saved best patch comparison for '{image_name}' to {output_path}\")\n",
    "        print(f\"Best Patch PSNR: EDSR = {highest_psnr:.2f} dB, Bilinear = {bilinear_psnr_best:.2f} dB\")\n",
    "\n",
    "def pad_image(image, patch_size):\n",
    "    \"\"\"Pad image to make dimensions divisible by patch size.\"\"\"\n",
    "    width, height = image.size\n",
    "    pad_width = (patch_size - width % patch_size) % patch_size\n",
    "    pad_height = (patch_size - height % patch_size) % patch_size\n",
    "    padding = (0, 0, pad_width, pad_height)\n",
    "    padded_image = pad(image, padding, fill=0)\n",
    "    return padded_image, padding\n",
    "\n",
    "# Run inference on a list of images\n",
    "\n",
    "perform_inference_single_patch_EDSR(\"chkt_v6/edsr_epoch_15.pth\", image_list, scale_factor=2, output_dir=\"edsr_best_patch_results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_direcotry = [\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0845.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0846.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0847.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\07848.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0859.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0845.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0846.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0847.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\07848.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0859.png\",    \n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0845.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0846.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0847.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\07848.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0859.png\",    \n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0845.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0846.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0847.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\07848.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0859.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0845.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0846.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0847.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\07848.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0859.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0847.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\07848.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0859.png\",    \n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0845.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0846.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0847.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\07848.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0859.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0845.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0846.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0847.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\07848.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0859.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0847.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\07848.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0859.png\",    \n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0845.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0846.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0847.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\07848.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0859.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0845.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0846.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0847.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\07848.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0859.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0847.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\07848.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0859.png\",    \n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0845.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0846.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0847.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\07848.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0859.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0845.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0846.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0847.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\07848.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\DIV2K\\0859.png\",\n",
    "    r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\HR_output.PNG\", r\"C:\\Users\\Turog\\OneDrive\\Documents\\GitHub\\576_DL_SuperRes\\data\\waves\\1702053632133310.png\"]\n",
    "perform_inference_comparison_EDSR(\"chkt_v6/edsr_epoch_15.pth\", image_list, scale_factor=2,output_dir=\"inference_results_v6\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
